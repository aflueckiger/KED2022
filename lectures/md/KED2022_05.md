---
title: The ABC of Computational Text Analysis
subtitle: "#5 Basic NLP with Command-line"
author: "Alex Flückiger"
institute: "Faculty of Humanities and Social Sciences<br>University of Lucerne" 
date: "31 March 2022"
lang: en-US
---

## Recap last Lecture

- perform shell commands :joystick:
  - navigate, create files
- complete assignment :writing_hand:



::: notes

- Einstieg in Shell
  - Verzeichnisbaum, Erstellen von Files/Ordner
  - Piping für komplexere Operationen
- Übungen ok? technische Fragen?
- letztes Mal inhaltliche Zumutung, heute erste inhaltlich interessante Analysen
- ähnliches Tempo, dafür mehr Zeit zum Üben

:::

## Outline

- corpus linguistic using the shell​ :knife:
  - counting, finding, comparing​​
- analyzing programmes of Swiss parties :bar_chart:



::: notes

- Frequenzanalysen = Schweizer Taschenmesser
- Ziel: mehr Übungszeit
- Syntax nicht merken, Wichtiges werdet ihr schlussendlich erinnern

:::

## When politics changes, <br>language changes.

![historical development of Swiss party politics ([Tagesanzeiger](https://blog.tagesanzeiger.ch/datenblog/index.php/1791/wie-sich-die-svp-aus-dem-buergerblock-verabschiedet-hat))](../images/swiss_party_politics.gif)



::: notes

- Gleiche Parteien, neue Ziele. Also doch nicht so gleich!
- Wie erkenne ich semantische Veränderungen?
  - hier: Wahlempfehlungen von Parteien
  - Welche Ziele/Ideologien stehen dahinter? --> Texte fundamental
- Wenn Politik ändert, ändert sich Sprache
  - oder gerade umgekehrtes zeitliches Verhältnis

:::



## Processing a Text Collection

- each document as individual file ​(​`​.​t​x​t​`​)​
  - use shell for quick analysis
- a dataset of documents (`.csv`, `.tsv`, `.xml`)
  - more effectively using Python?

![Processing a collection of documents ([src](https://unsplash.com/photos/5cFwQ-WMcJU))](../images/pile_books_unsplash.jpg)

::: notes

- txt-files erste Stufe bei Datensatzerstellung
- Daten existieren viele, Datensätze eher wenige
- bei Datensatz
  - Python praktischer
  - Dokument in Zelle in tsv/csv-file
- vorerst arbeiten wir nur mit txt files

:::



# Counting Things{data-background=../images/counting_blackboard.jpg }

## Frequency Analysis



:::::::::::::: {.columns}

:::{.column width="60%"}

* frequency ~ measure of relevance
* bag of words approach
* simple
* powerful

:::

:::{.column width="40%"}

![text as a bag of words ([src](https://cdn02.plentymarkets.com/r3pmentklgg2/item/../images/1716/full/Magnetwoerter.jpg))](../images/word_magnets.jpg)



:::

::::::::::::::



::: notes

- Häufigkeit indiziert Form von Relevanz
- in Häufigkeitsanalyse sind Worte kontextlos
  - BoW = Sack mit Wörtern
  - radikale Vereinfachung (einfaches Zählen) = grösster Vorteil (Verlust Ambiguitäten) = Nachteil
- theoetische Übersicht von Approaches später im Seminar
  - Kontrolle, was dahinter steht
- ähnlich wie Google Ngram, aber eigene Daten

:::

## Key Figures of Texts

```bash
wc *.txt	# count number of lines, words, characters
```



::: notes

- zuerst Charakterisierung Datenquelle, nicht nur Inhalt
- Zahlen für einzelne Dokumente und aggregiert auf Sammlung

:::

## Word Occurrences

#### show in context

```bash
grep -ir 'data'	folder/		# search in all files in folder, ignore case

# other grep options:
# --colour 		highlight pattern
# --context 2 	show 2 lines above/below match
```



#### count words

```bash
grep -ic 'data' */*.txt 	# count across all txt-files, ignore case
```



::: notes

- options
  - ignore case
  - recursive / specific files
- Dateinamen als Filter benutzen
  - Quelle/Jahr
  - grep -ir ' daten' `*svp*.txt`
- wc als Alternative
- zeige in Kurs-Repo
  - grep -irc --colour --context 3  'data'     lectures/*md | sort



:::



## Word Frequencies

#### steps of algorithm

1. split text into one word per line (tokenize)
2. sort words
3. count how often each word appears



```bash
# piping steps to get word frequencies
cat text.txt | tr ' ' '\n' | sort | uniq -c | sort -h > wordfreq.txt

# explanation of individual steps:
tr ' ' '\n' 	# replace spaces with newline 
sort -h			# sort lines alphanumerically
uniq -c			# count repeated lines
```



::: notes

- Zweck: Häufigkeiten aller Wörter
- kein direkter Befehl -> Kombinieren von Befehlen (modular)
- Befehle erklären
  - Zusammenfassen gleicher Zeilen mit uniq
- Aggregation extrem flexibel
  - anderer Text, alle Texte (*)
- Frage an Klasse: häufigstes Wort SVP? 
  - Schweiz, Bürger etc.: national, männlich

:::



## Word Frequencies

- absolute frequency
- relative frequency
  - `= occurrences / total_words`
  - independent of size
- statistical validation of variation
  - significance tests between corpora



::: notes

- Korpus = Textsammlung
- absolut nur, wenn grösserer Output (z.B. mehr Flyers) mitgemessen werden soll

:::



## Convert Stats into Dataset

- convert to tsv
- useful for further processing
  - export to Excel

```bash
# convert word frequencies into tsv-file
# additional step: replace a sequence of spaces with a tabulator
cat text.txt | tr ' ' '\n' | sort | uniq -c  | sort -h | \
tr -s ' ' '\t'  > test.tsv	
```

::: notes

- -s alle Leerschläge durch Tabulator ersetzen
- relative frequency in Excel

:::

# Preprocessing

## Common Preprocessing

### Refining results with

* lowercasing
* replace symbols
* join lines
* trimming header + footer
* splitting into multiple files
* using patterns to remove/extract parts :date:



::: notes

- Preprocessing für bessere Resultate

- Regex nächste Woche

:::



## Lowercasing

### reduce word forms

```bash
echo 'ÜBER' | tr "A-ZÄÖÜ" "a-zäöü"	# fold text to lowercase
```



::: notes

- Grossschreibung Satzanfang

:::

## Removing and Replacing Symbols

```bash
echo "3x3" | tr -d [:digit:]	# remove all digits	
cat text.txt | tr -d [:punct:]	# remove punctuation like .,:; 

tr 'Y' 'Z'						# replace any Y with Z
```



::: notes

- löscht alle Einzelzeichen in Text (keine Sequenzen)

::: 



## Standard Preprocessing

### save a preprocessed document

```bash
# lowercase, no punctuation, no digits
cat speech.txt | tr A-ZÄÖÜ a-zäöü | \
tr -d [:punct:] | tr -d [:digit:] > speech_clean.txt
```

::: notes

- Kleinschreibung , keine Interpunktion, keine Zahlen
- standardmässige Repräsentation in BoW (hier noch mit Reihenfolge)

:::

## Join Lines 

```bash
cat test.txt | tr -s '\n' ' '	# replace newlines with spaces
```



::: notes

- harte Zeilenumbrüche entfernen

:::

## Trim Lines

```bash
cat -n text.txt			# show line numbers
sed '1,10d' text.txt 	# remove the first 10 lines
```

## Splitting Files

```bash
# splits file at every delimiter into a stand-alone file
csplit huge_text.txt  "/delimiter/" {*}
```



## Check Differences between Files

#### sanity check after modification

```bash
# show differences side-by-side and only differing lines
diff -y --suppress-common-lines text_raw.txt text_proc.txt
```



## Where there is a shell,<br>there is a way. :thumbsup: {data-background=#4d7e65} 

::: notes

- Zusammenfassung
  - Nach Filesystem, nun auch Bearbeiten, Zählen 
- Shell =  flexibles + mächtiges Werkzeug

:::

# Questions?{data-background="../images/paint-anna-kolosyuk-unsplash.jpg" .white-text}

## In-class: Getting ready {data-background=#3c70b5}

1. Update your local copy of the GitHub repository KED2022 with `git pull`. 

   You find some party programmes (Grüne, SP, SVP) in `materials/data/party_programmes`. The programmes are provided in plain text which I have extracted from the official PDFs.
2. Fool around with individual commands to get a feeling of them before you proceed with the actual analysis.

## In-class: Analyzing Swiss Party Programmes I{data-background=#3c70b5}

1. Compare the absolute frequencies of single terms or multi-word expressions ...
   - across parties
   - historically within a party

   Use the file names as filter to get various aggregation of the word counts.

2. Pick terms of your interest and look at their contextual use by extracting relevant passages. Does the usage differ across parties or time?



**Share your insights with the class using [Etherpad](https://etherpad.wikimedia.org/p/KED2022).** 

## In-class: Analyzing Swiss Party Programmes II{data-background=#3c70b5}

1. Convert the word frequencies per party into a `tsv` dataset. Compute the relative word frequency from the absolute frequency using any spreadsheet software (e.g. Excel). Are your conclusions still valid after accounting for the size?
2. Can you refine the results with further preprocessing of the data?
3. Get the number of unique words rather than the total number of words of a document (piping).



**Pro Tip** :nerd_face:: Use `grep` to look up commands in the course slides

## Additional Resources

When you look for useful primers on Bash, consider the following resources:

- [Tutorial Basic Text Analysis by W. Turkel](https://williamjturkel.net/2013/06/15/basic-text-analysis-with-command-line-tools-in-linux/)
- [Tutorial Pattern Matching + KWIC by W. Turkel](https://williamjturkel.net/2013/06/20/pattern-matching-and-permuted-term-indexing-with-command-line-tools-in-linux/)

