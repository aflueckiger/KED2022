{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<section id=\"title-slide\">\n",
    "  <h1 class=\"title\">The ABC of Computational Text Analysis</h1>\n",
    "  <h2 class=\"subtitle\">#10: NLP with Python</h2>\n",
    "  <p class=\"author\">Alex Fl√ºckiger</p><p class=\"date\">12/19 May 2022</p>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "div.prompt {display:none}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview Analysis\n",
    "\n",
    "- get linguistic information from text\n",
    "- explore differences between two corpora \n",
    "    - using politcial party programmes\n",
    "- visualize term frequency over time\n",
    "  - using 1 August speeches by Swiss Federal Councillors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Do Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modules\n",
    "#### Standing of the Shoulders of Giants\n",
    "- [spaCy](https://spacy.io/usage/spacy-101): use or build state-of-the-art NLP pipeline\n",
    "- [textaCy](https://textacy.readthedocs.io): do high-level analysis, extends spaCy\n",
    "- [scattertext](https://github.com/JasonKessler/scattertext): visualize differences across corpora\n",
    "- [pandas](https://pandas.pydata.org/docs/getting_started/intro_tutorials/index.html): analyze tabular data \n",
    "- [plotnine](https://plotnine.readthedocs.io): visualize anything (*ggplot for Python*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Importing Modules\n",
    "\n",
    "various ways of importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# standard import\n",
    "import textacy\n",
    "import spacy\n",
    "\n",
    "# import with a short name\n",
    "import scattertext as st \n",
    "import pandas as pd\n",
    "\n",
    "# import all specific/all objects from a module\n",
    "from pathlib import Path\n",
    "from plotnine import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic NLP\n",
    "Process a single document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# example text (to read from a file see below)\n",
    "text = \"Apple's CEO Tim Cook is looking at buying U.K. startup for $1 billion.\"\n",
    "\n",
    "# load the English language model\n",
    "en = textacy.load_spacy_lang(\"en_core_web_sm\")\n",
    "\n",
    "# process document (tokenizing, tagging, parsing, recognizing named entities)\n",
    "doc = textacy.make_spacy_doc(text, lang=en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linguistic Features\n",
    "Features per token and their dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# visualize dependencies\n",
    "spacy.displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Get linguistic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# iterate over tokens of a document\n",
    "for token in doc:\n",
    "    print(token.text, \"-->\", token.lemma_, token.pos_,\n",
    "            token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# visualize named entities\n",
    "spacy.displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# iterate over named entities of a document\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} --> {ent.label_} ({spacy.explain(ent.label_)})\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Read from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# alternatively, read from a single txt file \n",
    "f_text = '../data/swiss_party_programmes/txt/sp_programmes/1920_parteiprogramm_d.txt'\n",
    "text = textacy.io.read_text(f_text)\n",
    "\n",
    "# show content\n",
    "# special generator syntax as text is read just-in-time (stream individual lines)\n",
    "print(next(text)[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Working with a Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Steps to create a Corpus\n",
    "\n",
    "How to make a corpus from many text files?\n",
    "\n",
    "1. list all files of a folder \n",
    "2. read text from each file\n",
    "3. parse metadata from file name\n",
    "4. return each document sequentially\n",
    "\n",
    "&rarr; wrap all this in a function `get_texts()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Define Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_texts(dir_texts):\n",
    "    \"\"\"\n",
    "    Sequentially stream all documents from a given folder, including metadata.\n",
    "    \"\"\"\n",
    "    p = Path(dir_texts) # set base directory\n",
    "    \n",
    "    # iterate over all documents in base directory\n",
    "    for fname in p.glob('**/*.txt'):\n",
    "        \n",
    "        print('Parsing file:', fname.name)\n",
    "        \n",
    "        text = next(textacy.io.text.read_text(fname))\n",
    "        # join lines as there are hard line-breaks\n",
    "        text = text.replace('\\n', ' ')\n",
    "        # further modify the text here if needed\n",
    "\n",
    "        # parse year from filename and set a metadata\n",
    "        # example: 1920_parteiprogramm_d.txt --> year=1920\n",
    "        try:\n",
    "            year = int(fname.name.split('_')[0])\n",
    "        except ValueError:\n",
    "            print('WARNING: Parsing meta data has failed:', fname.name)\n",
    "            continue\n",
    "\n",
    "        # add more metadata here if needed\n",
    "        metadata = {'fname': fname.name, 'year': year}\n",
    "        \n",
    "        # return documents one after another (sequentially)\n",
    "        yield (text, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create a Corpus from TXT\n",
    "Process documents and create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# stream texts from a given folder\n",
    "dir_texts = '../data/swiss_party_programmes/txt/sp_programmes/'\n",
    "texts = get_texts(dir_texts)\n",
    "\n",
    "# load German language model\n",
    "de = textacy.load_spacy_lang(\"de_core_news_sm\")\n",
    "\n",
    "# create corpus from processed documents\n",
    "corpus = textacy.Corpus(de, data=texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic Corpus Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('# documents:', corpus.n_docs)\n",
    "print('# sentences:', corpus.n_sents)\n",
    "print('# tokens:', corpus.n_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Export Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# get lowercased and filtered corpus vocabulary\n",
    "vocab = corpus.word_counts(by= 'lower_', weighting='count', filter_stops = True, filter_punct = True, filter_nums = True)\n",
    "\n",
    "# sort vocabulary by descending frequency\n",
    "vocab_sorted = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# write to file, one word and its frequency per line\n",
    "fname = '../analysis/vocab_frq.txt'\n",
    "with open(fname, 'w') as f:   \n",
    "    for word, frq in vocab_sorted:\n",
    "        line = f\"{word}\\t{frq}\\n\"\n",
    "        f.write(line)\n",
    "\n",
    "vocab_sorted[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Working with Subcorpus\n",
    "\n",
    "Interested in a group of documents only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# select the first document in corpus\n",
    "first_doc = corpus[0]\n",
    "first_doc._.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# function to filter by metadata, e.g. publication year after 1900\n",
    "def filter_func(doc):\n",
    "    return doc._.meta.get(\"year\") > 1900\n",
    "\n",
    "# create new corpus after applying filter function\n",
    "subcorpus = textacy.corpus.Corpus(de, data=corpus.get(filter_func))\n",
    "\n",
    "subcorpus.n_docs, corpus.n_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Key Word in Context (KWIC)\n",
    "\n",
    "show words in their original context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over documents and print matches\n",
    "# you can use regular expressions as keyword\n",
    "for doc in corpus:\n",
    "    results = textacy.extract.kwic.keyword_in_context(doc.text, keyword = '(Ausland|Inland)', ignore_case = True, window_width = 50)\n",
    "    for match in results:\n",
    "        print(f\"{match[0]}  {match[1]}  {match[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Export Results to File\n",
    "\n",
    "collect any information and write to file\n",
    "- particular terms\n",
    "- Named Entities (NE)\n",
    "- linguistic constructions\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for doc in corpus:\n",
    "    for sent in doc.sents:\n",
    "        if 'Armut' in sent.text:\n",
    "            # match contains the sentence where the term occurs, preceded by the filename (tab-separated)\n",
    "            match = f\"{doc._.meta['fname']}\\t{sent.text}\"\n",
    "            results.append(match)\n",
    "\n",
    "fname = '../analysis/sents_poverty.txt'\n",
    "with open(fname, 'w') as f:\n",
    "    f.write('\\n'.join(results))\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Export Corpus\n",
    "Save your dataset as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# merge metadata and actual content for each document in the corpus\n",
    "# ugly, verbose syntax to merge two dictionaries\n",
    "data = [{**doc._.meta, **{'text': doc.text}} for doc in corpus]\n",
    "\n",
    "# export corpus as csv\n",
    "f_csv = '../data/swiss_party_programmes/corpus_party_programmes.csv'\n",
    "textacy.io.csv.write_csv(data, f_csv, fieldnames=data[0].keys())\n",
    "\n",
    "# csv format is the best to load in scattertext\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# In-class: Exercises I\n",
    "\n",
    "1. Make sure that your local copy of the Github repository KED2022 is up-to-date with `git pull`. You can find the relevant material as follows:\n",
    "- notebook `materials/code/KED2022_10.ipynb`\n",
    "- party programmes `materials/data/swiss_party_programmes/txt`\n",
    "\n",
    "2. Open the notebook in VS Code. @Windows people: Make sure that you are connected to WSL Ubuntu (check green badge).\n",
    "\n",
    "3. Run all the code in the notebook by clicking `Run All`.\n",
    "\n",
    "4. Process a another English sentence with spaCy instead of the one mentioning Apple.\n",
    "\n",
    "5. Load the German language model and process a German sentence.\n",
    "\n",
    "6. Play around with the code as it is a good way to learn. Modify one thing, run the code, and see if the output matches your expectations. Start easy and then get increasingly brave until the code breaks. Fix the issue and try again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Explore Corpus interactively\n",
    "\n",
    "![Example Scattertext](../analysis/viz_party_differences.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scattertext\n",
    "\n",
    "- how does language differ by\n",
    "    - organization, person, gender, time etc.\n",
    "- interactive exploring\n",
    "- find discriminative terms \n",
    "    - *unigrams* + *bigrams*\n",
    "- scoring function *rank-frequency*\n",
    "    - normalized by number of terms `[0,1]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load CSV File\n",
    "\n",
    "load a dataset of 1 August speeches by Swiss federal councillors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# read dataset from csv file\n",
    "f_csv = '../data/dataset_speeches_federal_council_2019.csv'\n",
    "df = pd.read_csv(f_csv)\n",
    "\n",
    "# filter out non-german texts or very short texts\n",
    "df_sub = df[(df['Sprache'] == 'de') & (df['Text'].str.len() > 10)]\n",
    "\n",
    "# make new column containing all relevant metadata (showing in plot later on)\n",
    "df_sub['descripton'] = df_sub[['Redner', 'Partei', 'Jahr']].astype(str).agg(', '.join, axis=1)\n",
    "\n",
    "# sneak peek of dataset\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create Scattertext Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "censor_tags = set(['CARD']) # tags to ignore in corpus, e.g. numbers\n",
    "\n",
    "# stop words to ignore in corpus\n",
    "de_stopwords = spacy.lang.de.stop_words.STOP_WORDS # default stop words\n",
    "custom_stopwords = set(['[', ']', '%'])\n",
    "de_stopwords = de_stopwords.union(custom_stopwords) # extend with custom stop words\n",
    "\n",
    "# create corpus from dataframe\n",
    "# lowercased terms, no stopwords, no numbers\n",
    "# use lemmas for English only, German quality is too bad\n",
    "corpus_speeches = st.CorpusFromPandas(df_sub, # dataset\n",
    "                             category_col='Partei', # index differences by ...\n",
    "                             text_col='Text', \n",
    "                             nlp=de, # German model\n",
    "                             feats_from_spacy_doc=st.FeatsFromSpacyDoc(tag_types_to_censor=censor_tags, use_lemmas=False),\n",
    "                             ).build().get_stoplisted_unigram_corpus(de_stopwords)\n",
    "# produce visualization (interactive html)\n",
    "html = st.produce_scattertext_explorer(corpus_speeches,\n",
    "            category='SP', # set attribute to divide corpus into two parts\n",
    "            category_name='SP',\n",
    "            not_category_name='other parties',\n",
    "            metadata=df_sub['descripton'],\n",
    "            width_in_pixels=1000,\n",
    "            minimum_term_frequency=5, # drop terms occurring less than 5 times\n",
    "            save_svg_button=True,                          \n",
    ")\n",
    "\n",
    "# write visualization to html file\n",
    "fname = \"../analysis/viz_party_differences.html\"\n",
    "open(fname, 'wb').write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Plot Term Frequencies over Time\n",
    "\n",
    "![Example](../analysis/rel_term_frq_nation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create Corpus from CSV\n",
    "\n",
    "How to make a corpus from a dataset in `.csv`-format?\n",
    "\n",
    "&rarr; define a new function `get_texts_from_csv`, similar to `get_texts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_texts_from_csv(f_csv, text_column):\n",
    "    \"\"\"\n",
    "    Read dataset from a csv file and sequentially stream the rows,\n",
    "    including metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    # read dataframe\n",
    "    df = pd.read_csv(f_csv)\n",
    "    \n",
    "    # keep only documents that have text\n",
    "    filtered_df = df[df[text_column].notnull()]\n",
    "    \n",
    "    # iterate over rows in dataframe\n",
    "    for idx, row in filtered_df.iterrows():\n",
    "        \n",
    "        # read text and join lines (remove hard line-breaks)\n",
    "        text = row[text_column].replace('\\n', ' ')\n",
    "\n",
    "        # use all columns as metadata, except the column with the actual text\n",
    "        metadata = row.to_dict()\n",
    "        del metadata[text_column]\n",
    "\n",
    "        yield (text, metadata)\n",
    "\n",
    "f_csv = '../data/dataset_speeches_federal_council_2019.csv'\n",
    "texts = get_texts_from_csv(f_csv, text_column='Text')\n",
    "\n",
    "corpus_speeches = textacy.Corpus(de, data=texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create a Group-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# define what groups are formed and what terms should be included\n",
    "# here, groups by year and words are lowercased (incl. stop words)\n",
    "tokenized_docs, groups = textacy.io.unzip(\n",
    "        (textacy.extract.utils.terms_to_strings(textacy.extract.words(doc, filter_stops=False), by=\"lower\"),\n",
    "        doc._.meta[\"Jahr\"])\n",
    "        for doc in corpus_speeches)\n",
    "\n",
    "# define how to count\n",
    "# here relative term frequency\n",
    "vectorizer = textacy.representations.vectorizers.GroupVectorizer(\n",
    "        tf_type='linear', # absolute term frequency\n",
    "        dl_type=\"linear\", # normalized by document length\n",
    "        vocabulary_grps=range(1950, 2019)) # limit to years from 1950 to 2019\n",
    "\n",
    "# produce group-term-matrix with with frequency counts\n",
    "grp_term_matrix = vectorizer.fit_transform(tokenized_docs, groups)\n",
    "\n",
    "# create dataframe from matrix\n",
    "df_terms = pd.DataFrame.sparse.from_spmatrix(grp_term_matrix, index=vectorizer.grps_list, columns=vectorizer.terms_list)\n",
    "df_terms['year'] = df_terms.index.values\n",
    "\n",
    "# change shape of dataframe\n",
    "df_tidy = df_terms.melt(id_vars='year', var_name=\"term\", value_name=\"frequency\")\n",
    "df_tidy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Plot frequencies over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# filter the dataset for the following terms\n",
    "terms = [\"volk\", \"schweiz\", \"nation\"]\n",
    "df_terms = df_tidy[df_tidy['term'].isin(terms)]\n",
    "\n",
    "# plot the relative frequency for the terms above\n",
    "(ggplot(df_terms, aes(x='year', y='frequency', color='term'))\n",
    " + geom_point() # show individual points\n",
    " + stat_smooth(method='lowess', span=0.15, se=False) # overlay points with a smoothed line\n",
    " + theme_classic()) # make the plot look nicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# check some other terms\n",
    "terms = [\"solidarit√§t\", \"kultur\", \"werte\"]\n",
    "\n",
    "df_terms = df_tidy[df_tidy['term'].isin(terms)]\n",
    "\n",
    "(ggplot(df_terms, aes('year', 'frequency', color='term'))\n",
    " + geom_point(alpha=0.5, stroke = 0)\n",
    " + stat_smooth(method='lowess', span=0.1, se=False)\n",
    " + theme_classic())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Save Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# check some other terms\n",
    "terms = [\"schweizer\", \"schweizerinnen\"]\n",
    "df_terms = df_tidy[df_tidy['term'].isin(terms)]\n",
    "\n",
    "p = (ggplot(df_terms, aes('year', 'frequency', color='term'))\n",
    " + geom_point(alpha=0.5, stroke = 0) # set transparency\n",
    " + stat_smooth(method='lowess', span=0.2, se=False)\n",
    " + theme_classic())\n",
    "\n",
    "# save as png\n",
    "fname = '../analysis/rel_term_frq_gender.png'\n",
    "p.save(filename=fname, dpi=150, verbose = False)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Number of Documents per Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "docs_per_year = df_sub.groupby('Jahr').agg({'Text': \"count\" }).reset_index().rename(columns={'Text':'count'})\n",
    "\n",
    "(ggplot(docs_per_year, aes(x='Jahr', y='count'))\n",
    " + geom_line(color='darkblue')\n",
    " +  labs(title = \"Number of Speeches per Year\", x = \"Year\", y = \"absolute frequency\")\n",
    " + scale_y_continuous(breaks=range(0, 11))\n",
    " + theme_classic())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Working on Mini-Project\n",
    "\n",
    "Ask questions, <br>\n",
    "I am ready to help!\n",
    "\n",
    "![Help!](../../lectures/images/help_frog.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# In-class: Exercises I\n",
    "\n",
    "1. Make sure that your local copy of the Github repository KED2022 is up-to-date with `git pull`. You can find the relevant material as follows:\n",
    "- notebook `materials/code/KED2022_10.ipynb`\n",
    "- dataset `materials/data/dataset_speeches_federal_council_2019.csv`.\n",
    "\n",
    "2. Open the notebook in VS Code.\n",
    "\n",
    "3. Play around with the code as it is a good way to learn. Modify one thing, run the code, and see if the output matches your expectations. Start easy and then get increasingly brave until the code breaks. Fix the issue and try again.\n",
    "\n",
    "4. Getting bored? Check out the documentation of the packages linked on the next slide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Resources\n",
    "\n",
    "#### tutorials on spaCy\n",
    "\n",
    "- [official spaCy 101](https://spacy.io/usage/spacy-101)\n",
    "- [official online course spaCy](https://course.spacy.io/en/chapter1)\n",
    "- [Hitchhiker's Guide to NLP in spaCy](https://www.kaggle.com/nirant/hitchhiker-s-guide-to-nlp-in-spacy)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "rise": {
   "enable_chalkboard": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
